{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5b0ad5a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#First of all we define our map\n",
    "#we put D for dot/ E for empty/ G for ghost/ W for wall/ A for agent\n",
    "\n",
    "plan = [['W', 'W', 'W', 'W', 'W', 'W', 'W', 'W', 'W', 'W', 'W', 'W', 'W'],\n",
    "       ['W', 'W', 'W', 'W', 'W', 'W', 'W', 'W', 'W', 'W', 'W', 'W', 'W'],\n",
    "       ['W', 'W', 'E', 'D', 'D', 'D', 'D', 'D', 'D', 'D', 'D', 'W', 'W'],\n",
    "       ['W', 'W', 'D', 'W', 'W', 'W', 'D', 'W', 'W', 'W', 'D', 'W', 'W'],\n",
    "       ['W', 'W', 'D', 'W', 'D', 'D', 'D', 'D', 'D', 'W', 'D', 'W', 'W'],\n",
    "       ['W', 'W', 'D', 'D', 'D', 'W', 'E', 'W', 'D', 'D', 'D', 'W', 'W'],\n",
    "       ['W', 'W', 'D', 'W', 'D', 'W', 'E', 'W', 'D', 'W', 'D', 'W', 'W'],\n",
    "       ['W', 'W', 'D', 'W', 'D', 'D', 'W', 'D', 'D', 'W', 'D', 'W', 'W'],\n",
    "       ['W', 'W', 'D', 'D', 'D', 'D', 'D', 'D', 'D', 'D', 'D', 'W', 'W'],\n",
    "       ['W', 'W', 'W', 'W', 'W', 'W', 'W', 'W', 'W', 'W', 'W', 'W', 'W'],\n",
    "       ['W', 'W', 'W', 'W', 'W', 'W', 'W', 'W', 'W', 'W', 'W', 'W', 'W']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e977f416",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2657206\n",
      "2657206\n"
     ]
    }
   ],
   "source": [
    "#Here we build all states in which we have at most one ghost and agent is always in the middle\n",
    "#and we used backtracking to build it.\n",
    "#And we checked if it makes all possible states by counting all possible states manually and check it\n",
    "#with our states length.\n",
    "\n",
    "all_states = []\n",
    "def Build_all_states(i, cur_state, have_G):\n",
    "    if(i == 13):\n",
    "        all_states.append(cur_state)\n",
    "        return\n",
    "    \n",
    "    if(i == 6):\n",
    "        cur_state += 'A'\n",
    "        Build_all_states(i + 1, cur_state, have_G)\n",
    "        return\n",
    "    \n",
    "    possible_chars = ['D', 'E', 'W']\n",
    "    if(have_G == False):\n",
    "        possible_chars.append('G')\n",
    "    for char in possible_chars:\n",
    "        new_state = cur_state + char\n",
    "        Build_all_states(i + 1, new_state, (have_G or (char == 'G')))\n",
    "    return\n",
    "\n",
    "Build_all_states(0, \"\", False)\n",
    "all_states.append(\"L\")\n",
    "print(len(all_states))\n",
    "print((12 * (3 ** 11)) + 3 ** 12 + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9655cc10",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now it's time to define a function which finds next state\n",
    "#But first we need a function to develop random walk for our ghost\n",
    "#and then with finding our agent's and ghost's next location\n",
    "#we find if out agent loses or not and if it doesn't lose\n",
    "#we return it's new location with it's reward\n",
    "\n",
    "def find_next_G_loc(G_loc, cur_plan):\n",
    "    moves = ['L', 'R', 'U', 'D']\n",
    "    act = moves[np.random.randint(4)]\n",
    "    \n",
    "    new_G_loc = G_loc.copy()\n",
    "    if(act == 'R' and cur_plan[G_loc[0]][G_loc[1] + 1] != 'W'):\n",
    "        new_G_loc[1] += 1\n",
    "    elif(act == 'L' and cur_plan[G_loc[0]][G_loc[1] - 1] != 'W'):\n",
    "        new_G_loc[1] -= 1\n",
    "    elif(act == 'U' and cur_plan[G_loc[0] - 1][G_loc[1]] != 'W'):\n",
    "        new_G_loc[0] -= 1\n",
    "    elif(act == 'D' and cur_plan[G_loc[0] + 1][G_loc[1]] != 'W'):\n",
    "        new_G_loc[0] += 1\n",
    "    \n",
    "    return new_G_loc\n",
    "    \n",
    "\n",
    "def find_next_state(cur_plan, A_loc, G_loc, act):\n",
    "    new_A_loc = A_loc.copy()\n",
    "    if(act == 'R' and cur_plan[A_loc[0]][A_loc[1] + 1] != 'W'):\n",
    "        new_A_loc[1] += 1\n",
    "    elif(act == 'L' and cur_plan[A_loc[0]][A_loc[1] - 1] != 'W'):\n",
    "        new_A_loc[1] -= 1\n",
    "    elif(act == 'U' and cur_plan[A_loc[0] - 1][A_loc[1]] != 'W'):\n",
    "        new_A_loc[0] -= 1\n",
    "    elif(act == 'D' and cur_plan[A_loc[0] + 1][A_loc[1]] != 'W'):\n",
    "        new_A_loc[0] += 1\n",
    "    new_G_loc = find_next_G_loc(G_loc.copy(), cur_plan)\n",
    "    \n",
    "    if ((new_A_loc[0] == new_G_loc[0] and new_A_loc[1] == new_G_loc[1])\n",
    "        or ((A_loc[0] == new_G_loc[0] and A_loc[1] == new_G_loc[1]) \n",
    "            and (new_A_loc[0] == G_loc[0] and new_A_loc[1] == G_loc[1]))):\n",
    "        reward = -10\n",
    "        new_state = \"L\"\n",
    "        return new_state, reward, new_A_loc, new_G_loc, cur_plan\n",
    "    \n",
    "    reward = 0\n",
    "    if(cur_plan[new_A_loc[0]][new_A_loc[1]] == 'D'):\n",
    "        cur_plan[new_A_loc[0]][new_A_loc[1]] = 'E'\n",
    "        reward = 1\n",
    "        \n",
    "    new_state = \"\"\n",
    "    if(new_A_loc[0] - 2 == new_G_loc[0] and new_A_loc[1] == new_G_loc[1]):\n",
    "        new_state += 'G'\n",
    "    else:\n",
    "        new_state += cur_plan[new_A_loc[0] - 2][new_A_loc[1]]\n",
    "    for curi in range(new_A_loc[0] - 1, new_A_loc[0] + 2):\n",
    "        L = new_A_loc[1] - 1 - (curi == new_A_loc[0])\n",
    "        R = new_A_loc[1] + 2 + (curi == new_A_loc[0])\n",
    "        for curj in range(L, R):\n",
    "            if(curi == new_A_loc[0] and curj == new_A_loc[1]):\n",
    "                new_state += 'A'\n",
    "            elif(curi == new_G_loc[0] and curj == new_G_loc[1]):\n",
    "                new_state += 'G'\n",
    "            else:\n",
    "                new_state += cur_plan[curi][curj]\n",
    "    if(new_A_loc[0] + 2 == new_G_loc[0] and new_A_loc[1] == new_G_loc[1]):\n",
    "        new_state += 'G'\n",
    "    else:\n",
    "        new_state += cur_plan[new_A_loc[0] + 1][new_A_loc[1]]\n",
    "\n",
    "    return new_state, reward, new_A_loc, new_G_loc, cur_plan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f8d82a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here we hash our states into number from 0 to number of states\n",
    "#to achieve them faster and learn faster\n",
    "all_states_hash = [\"#\" for i in range(len(all_states))]\n",
    "def get_state_id(state):\n",
    "    cur = 0; B = 256; M = len(all_states)\n",
    "    for i in range(len(state)):\n",
    "        cur = ((cur * B) + ord(state[i])) % M\n",
    "        \n",
    "    while(all_states_hash[cur] != '#' and all_states_hash[cur] != state):\n",
    "        cur = (cur + 1) % M\n",
    "        \n",
    "    if(all_states_hash[cur] == '#'):\n",
    "        all_states_hash[cur] = state\n",
    "        \n",
    "    return cur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "36058e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here we define our Q function \n",
    "#We learn our Q function episodic and we do 30000 episodes to make sure our q function is learnt properly\n",
    "#we put a dynamic probability for it's movements\n",
    "#And after fixing each of it's movements we calculate it's next state and reward and base on these two\n",
    "#we update our Q table and also each 1000 episode, we print it's average achieved reward to check if it's converging\n",
    "\n",
    "import numpy as np\n",
    "def Qlearn(all_states, epsilon, alpha, gamma, episode_num, flag):\n",
    "    Q = np.zeros((len(all_states), 4))\n",
    "    action_space = ['L', 'R', 'U', 'D']\n",
    "\n",
    "    average_reward = 0\n",
    "    number_of_lose = 0\n",
    "    for episode in range(1, episode_num + 1):\n",
    "        if(epsilon > 0):\n",
    "            epsilon -= 1 / 20000\n",
    "\n",
    "        state = \"WWWWWWADDWDWD\"\n",
    "        state_id = get_state_id(state)\n",
    "        cur_plan = [[plan[i][j] for j in range(len(plan[0]))] for i in range(len(plan))]\n",
    "        loc_A = [2, 2]\n",
    "        loc_G = [6, 6]\n",
    "\n",
    "        for i in range(100):\n",
    "            if np.random.rand() < epsilon:\n",
    "                action = np.random.randint(4)\n",
    "            else:\n",
    "                action = np.argmax(Q[state_id, :])\n",
    "\n",
    "            next_state, reward, loc_A, loc_G, cur_plan = find_next_state(cur_plan, loc_A.copy(), loc_G.copy(), action_space[action])\n",
    "            next_state_id = get_state_id(next_state)\n",
    "\n",
    "            Q[state_id, action] = (1 - alpha) * Q[state_id, action] + alpha * (reward + gamma * np.max(Q[next_state_id, :]))\n",
    "\n",
    "            average_reward += reward\n",
    "            state = next_state\n",
    "            state_id = get_state_id(state)\n",
    "            if(state == 'L'):\n",
    "                number_of_lose += 1\n",
    "                break\n",
    "\n",
    "        if(episode % 1000 == 0):\n",
    "            average_reward /= 1000\n",
    "            if(flag):\n",
    "                print(\"episode\", episode)\n",
    "                print(\"average_reward\", average_reward)\n",
    "                print(\"number_of_lose\", number_of_lose)\n",
    "            if(episode != episode_num):\n",
    "                average_reward = 0\n",
    "                number_of_lose = 0\n",
    "    return Q, average_reward, number_of_lose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "78bba430",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 1000\n",
      "average_reward 7.332\n",
      "number_of_lose 482\n",
      "episode 2000\n",
      "average_reward 8.898\n",
      "number_of_lose 517\n",
      "episode 3000\n",
      "average_reward 11.323\n",
      "number_of_lose 503\n",
      "episode 4000\n",
      "average_reward 13.63\n",
      "number_of_lose 509\n",
      "episode 5000\n",
      "average_reward 15.84\n",
      "number_of_lose 512\n",
      "episode 6000\n",
      "average_reward 18.116\n",
      "number_of_lose 508\n",
      "episode 7000\n",
      "average_reward 19.484\n",
      "number_of_lose 512\n",
      "episode 8000\n",
      "average_reward 21.518\n",
      "number_of_lose 500\n",
      "episode 9000\n",
      "average_reward 23.739\n",
      "number_of_lose 484\n",
      "episode 10000\n",
      "average_reward 24.788\n",
      "number_of_lose 456\n",
      "episode 11000\n",
      "average_reward 26.192\n",
      "number_of_lose 444\n",
      "episode 12000\n",
      "average_reward 27.757\n",
      "number_of_lose 420\n",
      "episode 13000\n",
      "average_reward 29.153\n",
      "number_of_lose 397\n",
      "episode 14000\n",
      "average_reward 30.47\n",
      "number_of_lose 359\n",
      "episode 15000\n",
      "average_reward 32.405\n",
      "number_of_lose 314\n",
      "episode 16000\n",
      "average_reward 33.636\n",
      "number_of_lose 268\n",
      "episode 17000\n",
      "average_reward 34.838\n",
      "number_of_lose 243\n",
      "episode 18000\n",
      "average_reward 35.62\n",
      "number_of_lose 194\n",
      "episode 19000\n",
      "average_reward 37.73\n",
      "number_of_lose 116\n",
      "episode 20000\n",
      "average_reward 38.555\n",
      "number_of_lose 71\n",
      "episode 21000\n",
      "average_reward 39.893\n",
      "number_of_lose 24\n",
      "episode 22000\n",
      "average_reward 40.618\n",
      "number_of_lose 30\n",
      "episode 23000\n",
      "average_reward 41.548\n",
      "number_of_lose 17\n",
      "episode 24000\n",
      "average_reward 42.058\n",
      "number_of_lose 10\n",
      "episode 25000\n",
      "average_reward 38.935\n",
      "number_of_lose 19\n",
      "episode 26000\n",
      "average_reward 40.582\n",
      "number_of_lose 15\n",
      "episode 27000\n",
      "average_reward 41.811\n",
      "number_of_lose 3\n",
      "episode 28000\n",
      "average_reward 41.623\n",
      "number_of_lose 10\n",
      "episode 29000\n",
      "average_reward 41.714\n",
      "number_of_lose 11\n",
      "episode 30000\n",
      "average_reward 41.191\n",
      "number_of_lose 10\n"
     ]
    }
   ],
   "source": [
    "Q_table, average_reward, number_of_lose = Qlearn(all_states, 1, 0.5, 0.6, 30000, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "56057ebc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1 0.25 42.252 7\n",
      "0.1 0.5 41.972 8\n",
      "0.1 0.9 40.882 4\n",
      "0.1 1 6.307 1\n",
      "0.5 0.25 40.005 3\n",
      "0.5 0.5 41.647 2\n",
      "0.5 0.9 40.979 6\n",
      "0.5 1 5.902 0\n",
      "0.8 0.25 38.521 2\n",
      "0.8 0.5 38.293 3\n",
      "0.8 0.9 38.717 10\n",
      "0.8 1 4.379 5\n",
      "0.1 0.25 42.252\n"
     ]
    }
   ],
   "source": [
    "#Here we check which combination of alpha and gamma is the best for our model\n",
    "alpha_list = [0.1, 0.5, 0.8]\n",
    "gamma_list = [0.25, 0.5, 0.9, 1]\n",
    "\n",
    "max_average_reward = 0\n",
    "max_alpha = 0\n",
    "max_gamma = 0\n",
    "\n",
    "for alpha in alpha_list:\n",
    "    for gamma in gamma_list:\n",
    "        Q_table, average_reward, number_of_lose = Qlearn(all_states, 1, alpha, gamma, 30000, False)\n",
    "        print(alpha, gamma, average_reward, number_of_lose)\n",
    "        if(average_reward > max_average_reward):\n",
    "            max_average_reward = average_reward\n",
    "            max_alpha = alpha\n",
    "            max_gamma = gamma\n",
    "\n",
    "print(max_alpha, max_gamma, max_average_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "726d9a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here we build or adjancy list by saving transitions of our model learning again\n",
    "#in the adjancy_list variable which is a list of dictionaries and each dictionary is\n",
    "#for one of the states and it has for each of it's actions, values of next state and it's reward\n",
    "adjancy_list = [{} for i in range(len(all_states))]\n",
    "Q = np.zeros((len(all_states), 4))\n",
    "action_space = ['L', 'R', 'U', 'D']\n",
    "alpha = max_alpha\n",
    "gamma = max_gamma\n",
    "epsilon = 1\n",
    "episode_num = 30000\n",
    "\n",
    "average_reward = 0\n",
    "number_of_lose = 0\n",
    "for episode in range(1, episode_num + 1):\n",
    "    state = \"WWWWWWADDWDWD\"\n",
    "    state_id = get_state_id(state)\n",
    "    cur_plan = [[plan[i][j] for j in range(len(plan[0]))] for i in range(len(plan))]\n",
    "    loc_A = [2, 2]\n",
    "    loc_G = [6, 6]\n",
    "\n",
    "    for i in range(100):\n",
    "        if np.random.rand() < epsilon:\n",
    "            action = np.random.randint(4)\n",
    "        else:\n",
    "            action = np.argmax(Q[state_id, :])\n",
    "        next_state, reward, loc_A, loc_G, cur_plan = find_next_state(cur_plan, loc_A.copy(), loc_G.copy(), action_space[action])\n",
    "        adjancy_list[state_id][action_space[action]] = (next_state, reward)\n",
    "        next_state_id = get_state_id(next_state)\n",
    "        Q[state_id, action] = (1 - alpha) * Q[state_id, action] + alpha * (reward + gamma * np.max(Q[next_state_id, :]))\n",
    "        state = next_state\n",
    "        state_id = get_state_id(state)\n",
    "        if(state == 'L'):\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "5b93afde",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L ('WWWWWWADDWDWD', 0)\n",
      "R ('WWWWWEADDDWWW', 1)\n",
      "D ('WWEDWWAWWWDWD', 1)\n",
      "U ('WWWWWWADDWDWD', 0)\n"
     ]
    }
   ],
   "source": [
    "#Our start state's adjances and it's transition rewards\n",
    "print('L', adjancy_list[get_state_id(\"WWWWWWADDWDWD\")]['L'])\n",
    "print('R', adjancy_list[get_state_id(\"WWWWWWADDWDWD\")]['R'])\n",
    "print('D', adjancy_list[get_state_id(\"WWWWWWADDWDWD\")]['D'])\n",
    "print('U', adjancy_list[get_state_id(\"WWWWWWADDWDWD\")]['U'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "9b89df53",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here we learn our Q function again to save our Q table\n",
    "Q_table, average_reward, number_of_lose = Qlearn(all_states, 1, max_alpha, max_gamma, 30000, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "3c5e1e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here we save our Q table in 'Q_Table.txt' file\n",
    "np.savetxt('Q_Table.txt', Q_table, fmt='%d', delimiter=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "f6b5cde5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here we devise another plan to test our Q function in it\n",
    "new_plan = [['W', 'W', 'W', 'W', 'W', 'W', 'W', 'W', 'W', 'W', 'W', 'W', 'W'],\n",
    "            ['W', 'W', 'W', 'W', 'W', 'W', 'W', 'W', 'W', 'W', 'W', 'W', 'W'],\n",
    "            ['W', 'W', 'E', 'D', 'D', 'D', 'D', 'D', 'D', 'D', 'D', 'W', 'W'],\n",
    "            ['W', 'W', 'D', 'W', 'W', 'W', 'W', 'W', 'W', 'W', 'D', 'W', 'W'],\n",
    "            ['W', 'W', 'D', 'D', 'D', 'D', 'D', 'D', 'D', 'D', 'D', 'W', 'W'],\n",
    "            ['W', 'W', 'D', 'W', 'W', 'W', 'W', 'W', 'W', 'W', 'D', 'W', 'W'],\n",
    "            ['W', 'W', 'D', 'D', 'D', 'D', 'E', 'D', 'D', 'D', 'D', 'W', 'W'],\n",
    "            ['W', 'W', 'D', 'W', 'W', 'W', 'W', 'W', 'W', 'W', 'D', 'W', 'W'],\n",
    "            ['W', 'W', 'D', 'D', 'D', 'D', 'D', 'D', 'D', 'D', 'D', 'W', 'W'],\n",
    "            ['W', 'W', 'W', 'W', 'W', 'W', 'W', 'W', 'W', 'W', 'W', 'W', 'W'],\n",
    "            ['W', 'W', 'W', 'W', 'W', 'W', 'W', 'W', 'W', 'W', 'W', 'W', 'W']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "07ec65a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28\n",
      "29\n",
      "6\n",
      "15\n",
      "5\n",
      "28\n",
      "27\n",
      "12\n",
      "13\n",
      "9\n",
      "Average =  17.2\n"
     ]
    }
   ],
   "source": [
    "#And here we move on our new plan and save how much reward we achieve\n",
    "#Also we use a probiblity distirbution for deciding our action again\n",
    "#To don't get stuck in new states.\n",
    "total_sum = 0\n",
    "for i in range(10):\n",
    "    action_space = ['L', 'R', 'U', 'D']\n",
    "    reward_sum = 0\n",
    "    state = \"WWWWWWADDWDWD\"\n",
    "    state_id = get_state_id(state)\n",
    "    cur_plan = [[new_plan[i][j] for j in range(len(plan[0]))] for i in range(len(plan))]\n",
    "    loc_A = [2, 2]\n",
    "    loc_G = [6, 6]\n",
    "    epsilon = 0.3\n",
    "\n",
    "    while(state != \"L\" and reward_sum < 40):\n",
    "        if np.random.rand() < epsilon:\n",
    "            action = np.random.randint(4)\n",
    "        else:\n",
    "            action = np.argmax(Q[state_id, :])\n",
    "\n",
    "        next_state, reward, loc_A, loc_G, cur_plan = find_next_state(cur_plan, loc_A.copy(), loc_G.copy(), action_space[action])\n",
    "        next_state_id = get_state_id(next_state)\n",
    "\n",
    "        if(reward > 0):\n",
    "            reward_sum += reward\n",
    "        state = next_state\n",
    "        state_id = get_state_id(state)\n",
    "\n",
    "    print(reward_sum)\n",
    "    total_sum += reward_sum\n",
    "print(\"Average = \", total_sum / 10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
